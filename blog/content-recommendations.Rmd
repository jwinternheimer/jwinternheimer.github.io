---
date: 2017-07-07T10:01:22-04:00
author: Julian Winternheimr
type: "post"
tags: []
title: "Simple Content Recommendations in R"
---

I've wanted to build a content recommendation engine for a long time. Buffer is sitting on a mountain of data, which some argue is [the most valuable commodity on Earth](http://subprint.com/blog/data-is-the-most-valuable-commodity-on-earth). We used to provide content suggestions to users through manual curation, but [we ended up retiring that feature/service](https://open.buffer.com/retiring-suggestions/). 

In this analysis we'll build a simple recommendation engine with the `recommenderlab` R package, using collaborative filtering techniques. To keep it simple, we'll only look at updates shared by Buffer team members.

### Collaborative filtering
The basic idea behind collaborative filtering is that, if two people share similar interests (e.g. they've shared the same links), they will also share similar interests in the future. For example, if Rupert and I have similar histories in terms of the links that we've shared, it would make sense to recommend links that Rupert has shared (that I haven't).

In this type of recommendation, filtering items from a large set of alternatives is done collaboratively between users preferences. Such systems are called collaborative filtering recommender systems.

There are two sub-branches of collaborative filtering that are worth mentioning:

 - **Item-based**: This recommends to a user the items that are most similar to the user's purchases.
 
 - **User-based**: This recommends to a user the items that are the most preferred by similar users.
 
In this analysis we'll use _user-based_ collaborative filtering. With this method, ratings (or recommendations) for a user can be predicted by first finding a neighborhood of similar users and then aggregate the ratings of these users to form a prediction.

The big disadvantage of this approach is scalability since the whole user database has to be processed. This is why we're starting with a small dataset.

The neighborhood is defined in terms of similarity between users, either by taking a given
number of most similar users (k nearest neighbors) or all users within a given similarity
threshold. Popular similarity measures for CF are the Pearson correlation coefficient and the
Cosine similarity.

We want to create recommendations for people based on the preferences of people with similar interests. To find the k-neighborhood (i.e., the k nearest neighbors) we calculate the similarity
between the user and all other users based on their ratings (1 for a shared url, 0 otherwise) in the dataset and then select the k users with the highest similarity.

![](http://i.imgur.com/h0NvZvY.png)
 
### Data mining and EDA
It's worth noting that I'm not doing any exploratory analysis in this analysis. It's Friday and I'm getting a little lazy. :) However this should absolutely be done at some point to give us a better underlying understanding of the dataset. 

```{r include = FALSE}
library(dplyr); library(ggplot2); library(tidyr); library(stringr); library(buffer)
```

## Data collection
Let's collect all of the links shared by Buffer team members. The dataset will only include two columns, `name` and `url`. 

```{r}
# Read csv of updates
updates <- read.csv("~/Downloads/buffer_updates.csv", header = TRUE)
```

Alright, we have over 98 thousand links from 72 team members.

## Data tidying
We need to clean up the column names and set the correct object types.

```{r}
# Rename the columns
colnames(updates) <- c('name', 'url')

# Set urls as character vector
updates$url <- as.character(updates$url)
```

Now we need to clean up the links. We'll use the `gsub()` function and some regex to remove all characters after `?` in the urls.

```{r}
updates$clean_url <- gsub("\\?.*", "", updates$url)
```

Nice. There are still some unique urls that we'd like to remove. We can do that manually.

```{r}
# Define the urls we want to exclude
problem_urls <- c("nytimes.com/glogin", "buffer.com", "buffer.com/respond", "nytimes.com",
                  "facebook.com/photo.php", "myaccount.nytimes.com/auth/login", "google.com",
                  "bufferapp.com", "news.ycombinator.com/item")

# Filter out these urls
updates <- updates %>%
  filter(!(clean_url %in% problem_urls))
```

Alright, now we're ready to build our recommender!

## Recommender
The `recommenderlab` package can be used for collaborative filtering. To get started, we need to change the structure of our dataset to a wide matrix, which columns representing urls and rows representing users.

Our data is binary (i.e. either 1 or 0). If a user shared the url, then it's a 1, otherwise it's 0. Recommender systems using association rules produce recommendations based on a dependency model for items given by a set of association rules.

The binary profile matrix `R` is seen as a dataset where each user is treated as a transaction that contains the subset of items in _I_ with a rating of 1 (when the user has shared the link). Hence transaction _k_ is defined as `Tk = {ij ∈ I|rjk = 1}`, which indicates whether a link is an element of _I_ or the rating of that specific link is 1.

The whole transaction dataset `D = {T1, T2, . . . , TU }` where U is the number of users. This is a wide matrix that we have to build, that includes a rating for each user and each link.

To build the dependency model, a set of association rules is mined from the matrix. Association rules are rules of the form `X → Y` where `X , Y ⊆ I` and `X ∩ Y = ∅.` This means that the links `X` and `Y` are a subset of the entire list of links `I` and the intersection of `X` and `Y` is 0 (the user hasn't shared both).

To select a set of useful association rules, thresholds on measures of significance and interestingness are used. Two widely applied measures are:

 - `support(X → Y) = support(X ∪ Y) = Freq(X ∪ Y)/|D| = P(EX ∩ EY )`
 
 - `confidence(X → Y) = support(X ∪ Y)/support(X ) = P(EY |EX )`

`Freq(I)` gives the number of transactions in the data base `D` that contains all items in `I`. `EX`
is the event that the item X is contained in a transaction. So `support(X → Y)` is the probability that link X _or_ link Y is shared, and `confidence(X → Y)` is the probability that link Y is shared _given that link X was shared_.

To make a recommendation for an active user ua given the set of items `I` the user likes and the set of association rules `R` (dependency model), the following steps are necessary:

  1. Find all matching rules `X → Y`.
  2. Recommend N unique right-hand-sides (Y) of the matching rules with the highest con-fidence (or another measure of interestingness).
  
Let's build our binary profile matrix.

```{r include = FALSE}
library(recommenderlab); library(reshape2); library(methods)
```

```{r warning = FALSE, message = FALSE}
# Cast as a giant matrix
updates_cast <- acast(updates, name ~ clean_url)
umatrix <- as.matrix(updates_cast)

# Replace NA values with 0
umatrix[is.nan(umatrix)] <- 0

# Set as a binary rating matrix type
binary_matrix = as(umatrix, 'binaryRatingMatrix')

binary_matrix
```

We now have a 72 by 78203 matrix that we can work with.

The package comes with a lot of nifty built-in algorithms that can be used for both rating (e.g. 1-5 starts) and binary (0 or 1) data sets. The supported algoritms are:

 - User-based collborative filtering (UBCF)
 - Item-based collborative filtering (IBCF)
 - SVD with column-mean imputation (SVD)
 - Funk SVD (SVDF)
 - Association rule-based recommender (AR)
 - Popular items (POPULAR)
 - Randomly chosen items for comparison (RANDOM)
 - Re-recommend liked items (RERECOMMEND)
 - Hybrid recommendations (HybridRecommender)
 
We'll use the `UBCF` algorithm in this analysis, and we won't use the best practices of splitting the data into training and testing sets. We'll only use the eyeball test to evaluate the model's performance.

To generate an aggregated estimated rating, we compute the average ratings in the neighborhood for each url not shared by the user. To create a top-N recommendation list, the urls are ordered by predicted rating. The fact that some users in the neighborhood are more similar to the active user than others can be incorporated as weights. 

```{r}
# Build user-based collaborative filtering recommender
recommender <- Recommender(binary_matrix, method = "UBCF")

# Make top 10 recommendation list for each user
predictions <- predict(recommender, binary_matrix, type = "topNList")
```

Great, now let's convert the predictions to a list and view the results!

```{r}
# Convert prediction into list, user-wise
pred_list <- as(predictions, "list")
pred_list
```

It looks like there is quite a lot of overlap, and this makes sense. We all work for the same company and share similar things. I imagine that our similarity scores are all quite high. Notice all of the buffer blog posts in there. :) 

## Next steps
In the future I would love to build a recommender with a much larger dataset with a wider range of users. Proper training/testing and evaluation methods should also be utilized. I would also love to utilize time as a factor -- I would be more likely to share a new article than an old one, so that could be factored in as well.




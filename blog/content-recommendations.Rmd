---
date: 2017-07-07T10:01:22-04:00
author: Julian Winternheimr
type: "post"
tags: []
title: "Simple Content Recommendations in R"
---

I've wanted to build a content recommendation engine for a long time. Buffer is sitting on a mountain of data, which some argue is [the most valuable commodity on Earth](http://subprint.com/blog/data-is-the-most-valuable-commodity-on-earth). We used to provide content suggestions to users through manual curation, but [we ended up retiring that feature/service](https://open.buffer.com/retiring-suggestions/). 

In this analysis we'll build a simple recommendation engine with the `recommenderlab` R package, using collaborative filtering techniques. To keep it simple, we'll only look at updates shared by Buffer team members.

### Collaborative filtering
The basic idea behind collaborative filtering is that, if two people share similar interests (e.g. they've shared the same links), they will also share the same similar interests in the future. For example, if Rupert and I have similar histories in terms of the links that we've shared, it would make sense to suggests links that Rupert has shared (that I haven't) to me.

In this type of recommendation,  ltering items from a large set of alternatives is done collaboratively between users preferences. Such systems are called collaborative  ltering recommender systems.

There are two sub-branches of collaborative filtering that are worth mentioning:

 - **Item-based**: This recommends to a user the items that are most similar to the user's purchases.
 - **User-based**: This recommends to a user the items that are the most preferred by similar users.
 
### Data mining and EDA
It's worth noting that I'm not doing any exploratory analysis in this analysis. It's Friday and I'm getting a little lazy. :) However this should absolutely be done at some point to give us a better underlying understanding of the dataset. 

```{r include = FALSE}
library(dplyr); library(ggplot2); library(tidyr); library(stringr); library(buffer)
```

## Data collection
Let's collect all of the links shared by Buffer team members. The dataset will only include two columns, `name` and `url`. 

```{r}
# Read csv of updates
updates <- read.csv("~/Downloads/buffer_updates.csv", header = TRUE)
```

Alright, we have 98 thousand links from 72 team members!

## Data tidying
We need to clean up the column names and set the correct object types.

```{r}
# Rename the columns
colnames(updates) <- c('name', 'url')

# Set urls as character vector
updates$url <- as.character(updates$url)
```

Now we need to clean up the links. We'll use the `gsub()` function and some regex to remove all characters after `?` in the urls.

```{r}
updates$clean_url <- gsub("\\?.*", "", updates$url)
```

Nice. There are still some unique urls that we'd like to remove. We can do that manually.

```{r}
# Define the urls we want to exclude
problem_urls <- c("nytimes.com/glogin", "buffer.com", "buffer.com/respond", "nytimes.com",
                  "facebook.com/photo.php", "myaccount.nytimes.com/auth/login", "google.com",
                  "bufferapp.com", "news.ycombinator.com/item")

# Filter out these urls
updates <- updates %>%
  filter(!(clean_url %in% problem_urls))
```

Alright, now we're ready to build our recommender!

## Recommender
The `recommenderlabd` package can be used for collaborative filtering. To get started, we need to change the structure of our dataset to a wide matrix, which columns representing urls and rows representing users.

Our data is binary (i.e. either 1 or 0). If a user shared the url, then it's a 1, otherwise it's 0. We'll use the `reshape2` package to restructure the data.

```{r include = FALSE}
library(recommenderlab); library(reshape2); library(methods)
```

```{r warning = FALSE, message = FALSE}
# Cast as a giant matrix
updates_cast <- acast(updates, name ~ clean_url)
umatrix <- as.matrix(updates_cast)

# Set as a binary rating matrix
binary_matrix = as(umatrix, 'binaryRatingMatrix')

binary_matrix
```

We now have a 72 by 78203 matrix that we can work with.

The package comes with a lot of nifty built-in algorithms that can be used for both rating (e.g. 1-5 starts) and binary (0 or 1) data sets. The supported algoritms are:

 - User-based collborative filtering (UBCF)
 - Item-based collborative filtering (IBCF)
 - SVD with column-mean imputation (SVD)
 - Funk SVD (SVDF)
 - Association rule-based recommender (AR)
 - Popular items (POPULAR)
 - Randomly chosen items for comparison (RANDOM)
 - Re-recommend liked items (RERECOMMEND)
 - Hybrid recommendations (HybridRecommender)
 
We'll use the `UBCF` algorithm in this small analysis, and we won't use the best practices of splitting the data into training and testing sets. We'll only use the eyeball test to evaluate the model's performance.

```{r}
# Build user-based collaborative filtering recommender
recommender <- Recommender(binary_matrix, method = "UBCF")

# Make top 10 recommendation list for each user
predictions <- predict(recommender, binary_matrix, type = "topNList")
```

Great, now let's convert the predictions to a list and view the results!

```{r}
# Convert prediction into list, user-wise
pred_list <- as(predictions, "list")
pred_list
```

It looks like there is **a lot** of overlap, and this makes sense. We all work for the same company and share similar things. I imagine that our similarity scores are all quite high. Notice all of the buffer blog posts in there. :) 

## Next steps
In the future I would love to build a recommender with a much larger dataset with a wider range of users. Proper training/testing and evaluation methods should also be utilized. I would also love to utilize time as a factor -- I would be more likely to share a new article than an old one, so that could be factored in as well.




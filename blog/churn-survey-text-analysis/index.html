<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Why Users Churn: A Text Analysis of Churn Surveys</title>
  <meta property="og:title" content="Why Users Churn: A Text Analysis of Churn Surveys" />
  <meta name="twitter:title" content="Why Users Churn: A Text Analysis of Churn Surveys" />
  <meta name="description" content="People decide to leave or stop paying for Buffer every day. It’s unfortunate, but it happens for one reason or another.
We collect a lot of data from these users in the form of surveys. We thought that it might be beneficial to analyze the text of these survey comments to see if we can identify common themes that we could address.
Data collection We’ll use data collected from four separate surveys that represents different types of churn:">
  <meta property="og:description" content="People decide to leave or stop paying for Buffer every day. It’s unfortunate, but it happens for one reason or another.
We collect a lot of data from these users in the form of surveys. We thought that it might be beneficial to analyze the text of these survey comments to see if we can identify common themes that we could address.
Data collection We’ll use data collected from four separate surveys that represents different types of churn:">
  <meta name="twitter:description" content="People decide to leave or stop paying for Buffer every day. It’s unfortunate, but it happens for one reason or another.
We collect a lot of data from these users in the form of surveys. We thought …">
  <meta name="author" content="Julian Winternheimer"/>
  <link href='/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta property="og:image" content="/img/julian-avatar.png" />
  <meta name="twitter:image" content="/img/julian-avatar.png" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@julheimer" />
  <meta name="twitter:creator" content="@julheimer" />
  <meta property="og:url" content="/blog/churn-survey-text-analysis/" />
  <meta property="og:type" content="website" />
  <meta property="og:site_name" content="Data Analysis?" />

  <meta name="generator" content="Hugo 0.23" />
  <link rel="canonical" href="/blog/churn-survey-text-analysis/" />
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Data Analysis?">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="/css/pygment_highlights.css" />
  <link rel="stylesheet" href="/css/highlight.min.css" />




<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.css" integrity="sha256-sCl5PUOGMLfFYctzDW3MtRib0ctyUvI9Qsmq2wXOeBY=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/default-skin/default-skin.min.css" integrity="sha256-BFeI1V+Vh1Rk37wswuOYn5lsTcaU96hGaI7OUVCLjPc=" crossorigin="anonymous" />



<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

</head>

  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Data Analysis?</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" href="javascript:void(0)">Samples</a>
              <div class="navlinks-children">
                
                  <a href="/post/2017-03-07-bigimg-sample">Big Image Sample</a>
                
                  <a href="/post/2017-03-05-math-sample">Math Sample</a>
                
              </div>
            </li>
          
        
          
            <li>
              <a title="About" href="/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    <div class="avatar-container">
      <div class="avatar-img-border">
        
          <a title="Data Analysis?" href="/">
            <img class="avatar-img" src="/img/julian-avatar.png" alt="Data Analysis?" />
          </a>
        
      </div>
    </div>

  </div>
</nav>




    
  
  
  




  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              <h1>Why Users Churn: A Text Analysis of Churn Surveys</h1>
                
                
                  <span class="post-meta">
  Posted on July 8, 2017
  
</span>


                
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>People decide to leave or stop paying for Buffer every day. It’s unfortunate, but it happens for one reason or another.</p>
<p>We collect a lot of data from these users in the form of surveys. We thought that it might be beneficial to analyze the text of these survey comments to see if we can identify common themes that we could address.</p>
<div id="data-collection" class="section level2">
<h2>Data collection</h2>
<p>We’ll use data collected from four separate surveys that represents different types of churn:</p>
<ul>
<li>The <em>exit survey</em> prompts users to explain why they are abandoning the Buffer product completely.</li>
<li>The <em>business churn survey</em> prompts users to explain why they are canceling their business subscriptions.</li>
<li>The <em>awesome downgrade survey</em> prompts users to explain why they are canceling their awesome subscriptions.</li>
<li>The <em>business downgrade awesome survey</em> asks why users downgrade from a Business to an Awesome subscription.</li>
</ul>
<p>We’ve gathered the data in <a href="https://looker.buffer.com/looks/3949">this look</a>. We can use the <code>get_look()</code> function from the <code>buffer</code> package to import all of the survey responses into an R dataframe.</p>
<pre class="r"><code># Get churn responses
responses &lt;- get_look(3949)</code></pre>
<p>Great, we have over 30,000 survey responses! We’ll need to clean the data up a bit to get it ready for analysis.</p>
<pre class="r"><code>library(lubridate)</code></pre>
<pre><code>## Loading required package: methods</code></pre>
<pre><code>## 
## Attaching package: &#39;lubridate&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     date</code></pre>
<pre class="r"><code># Rename columns
colnames(responses) &lt;- c(&#39;created_at&#39;, &#39;user_id&#39;, &#39;type&#39;, &#39;reason&#39;, &#39;specifics&#39;, &#39;details&#39;)

# Set strings as character type
responses$details &lt;- as.character(responses$details)

# Set date as date
responses$created_at &lt;- as.Date(responses$created_at, format = &#39;%Y-%m-%d&#39;)

# Get the month
responses &lt;- responses %&gt;%
  mutate(month = as.Date(paste0(format(created_at, &quot;%Y-%m&quot;), &#39;-01&#39;), format = &#39;%Y-%m-%d&#39;))

# Remove the respon and specifics columns
responses$reason &lt;- NULL
responses$specifics &lt;- NULL

# Remove NA values
responses &lt;- responses %&gt;%
  filter(details != &quot;&quot; &amp; details != &#39;[No reason supplied]&#39; &amp; details != &#39;false&#39;)</code></pre>
<p>After cleaning the data, we still have around 16 thousand responses from November 2015 to July 2017 to work with.</p>
</div>
<div id="data-tidying" class="section level2">
<h2>Data tidying</h2>
<p>Here is some context on the idea of tidy data taken from the book Tidy Text Mining with R:</p>
<blockquote>
<p>Using tidy data principles is a powerful way to make handling data easier and more effective, and this is no less true when it comes to dealing with text. As described by Hadley Wickham (Wickham 2014), tidy data has a specific structure: - Each variable is a column - Each observation is a row - Each type of observational unit is a table</p>
</blockquote>
<p>We thus define the tidy text format as being a table with one-token-per-row. A token can be a word or an n-gram. Within our tidy text framework, we need to both break the comments into individual tokens and transform it to a tidy data structure.</p>
<p>To do this, we use tidytext’s <code>unnest_tokens()</code> function. This breaks the churn survey responses into individual words and includes one word per row while retaining the attributes (survey type, user_id, etc) of that word.</p>
<pre class="r"><code># Unnest the tokens
text_df &lt;- responses %&gt;%
  unnest_tokens(word, details)</code></pre>
<p>Now that the data is in one-word-per-row format, we can manipulate it with tidy tools like <code>dplyr</code>. Often in text analysis, we will want to remove stop words; stop words are words that are not useful for an analysis, typically extremely common words such as “the”, “of”, “to”, and so forth in English. We can remove stop words (kept in the tidytext dataset stop_words) with an <code>anti_join()</code>.</p>
<pre class="r"><code># Collect stop words
data(stop_words)

# Remove stop words from our dataset with an anti_join()
text_df &lt;- text_df %&gt;%
  anti_join(stop_words, by = &quot;word&quot;)</code></pre>
<p>Great! We’ve got a tidy data frame now.</p>
</div>
<div id="data-exploration" class="section level2">
<h2>Data exploration</h2>
<p>Let’s take a moment here to see the most common words overall from the churn surveys.</p>
<pre class="r"><code># Find most common words
text_df %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 200) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n)) +
  geom_col() +
  labs(x = &quot;&quot;, y = &quot;&quot;, title = &quot;Most Common Words&quot;) + 
  coord_flip()</code></pre>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>It’s interesting to see “plan” used so frequently. Words like “post”, “time”, and “facebook” may be possible signals as well. It’s nice to see “love” in there as well.</p>
<p>Ok, we can now look for the words that occur more frequently in the business churn survey, relative to other surveys.</p>
<p>To find these words, we can calculate the relative frequency of words that appear in the business churn survey and compare that to the relative frequency of the words in the other surveys.</p>
<pre class="r"><code>library(tidyr)

# Calculate relative frequency of words
frequency &lt;- text_df %&gt;%
  filter(!(is.na(type)) &amp; type != &quot;&quot;) %&gt;%
  count(type, word) %&gt;%
  group_by(type) %&gt;%
  mutate(proportion = n / sum(n)) %&gt;% 
  select(-n) %&gt;% 
  spread(type, proportion) %&gt;% 
  gather(segment, proportion, 
         c(awesome_downgrade_survey, business_downgrade_awesome_survey:exit_survey))

# Replace NA with 0
frequency[is.na(frequency)] &lt;- 0</code></pre>
<p>Now we can plot the relative frequencies of popular words, to help us visualize the relative frequencies.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Words that are close to the dotted line in these plots have similar frequencies in both sets of comments. For example, in both the business churn survey and the awesome downgrade survey, the terms “accounts”, “love”, and “afford” are used frequently.</p>
<p>Words that are far from the line are words that are found more in one set of comments than another. Words on the left side of the dotted line occur more frequently in the business churn survey’s comments than in the other surveys. For example, in the <code>awesome_downgrade_survey</code> panel, words like “expensive”, “reporting”, and “analytcs” are much more common in the business churn survey than in the awesome survey.</p>
<p>In the <em>exit survey</em>, words like “free”, “wrong”, “email”, and “connected” appear more frequently than in the <em>business churn survey</em>.</p>
</div>
<div id="diving-deeper-into-word-frequency" class="section level2">
<h2>Diving deeper into word frequency</h2>
<p>Another way to analyze a term’s relative frequency is to calculate the <em>inverse document frequency (tdf)</em>, which is defined as:</p>
<p><code>idf(term) = ln(documents / documents containing term)</code></p>
<p>A term’s inverse document frequency (idf) decreases the weight for commonly used words and increases the weight for words that are not used very much. This can be combined with term frequency to calculate a term’s <code>tf-idf</code> (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.</p>
<p>The idea of tf-idf is to find the important words for the content of each collection of words (the surveys being the collections of words) by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in an entire collection of documents, in this case the text of all of the surveys combined.</p>
<p>The <code>bind_tf_idf</code> function takes a tidy text dataset as input with one row per token (word), per document. One column (<code>word</code> here) contains the terms, one column contains the documents (<code>type</code> here), and the last necessary column contains the counts, how many times each document contains each term (<code>n</code>).</p>
<pre class="r"><code># Calculate the frequency of words for each survey
survey_words &lt;- text_df %&gt;%
  count(type, word, sort = TRUE) %&gt;%
  ungroup()

# Calculate the total number of words for each survey
total_words &lt;- survey_words %&gt;% 
  group_by(type) %&gt;% 
  summarize(total = sum(n))

# Join the total words back into the survey_words data frame
survey_words &lt;- left_join(survey_words, total_words, by = &quot;type&quot;) %&gt;%
  filter(type != &quot;&quot;)

# View data 
head(survey_words)</code></pre>
<pre><code>## # A tibble: 6 x 4
##                       type    word     n total
##                     &lt;fctr&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt;
## 1              exit_survey account  1142 16680
## 2 awesome_downgrade_survey  buffer   890 22688
## 3              exit_survey  buffer   572 16680
## 4 awesome_downgrade_survey  social   474 22688
## 5 awesome_downgrade_survey    plan   469 22688
## 6 awesome_downgrade_survey   media   358 22688</code></pre>
<p>There is one row in this data frame for each word-survey combination. <code>n</code> is the number of times that word is used in that survey and total is the total number of words in the survey responses.</p>
<div id="the-bind_tf_idf-function" class="section level3">
<h3>The <code>bind_tf_idf</code> function</h3>
<p>The idea of tf-idf is to find the important words for the content of each collection of comments by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in an entire collection of documents, in this case all survey responses.</p>
<pre class="r"><code># Calculate tf_idf
survey_words &lt;- survey_words %&gt;%
  bind_tf_idf(word, type, n)

head(survey_words)</code></pre>
<pre><code>## # A tibble: 6 x 7
##                       type    word     n total         tf       idf
##                     &lt;fctr&gt;   &lt;chr&gt; &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;
## 1              exit_survey account  1142 16680 0.06846523 0.2231436
## 2 awesome_downgrade_survey  buffer   890 22688 0.03922779 0.2231436
## 3              exit_survey  buffer   572 16680 0.03429257 0.2231436
## 4 awesome_downgrade_survey  social   474 22688 0.02089210 0.2231436
## 5 awesome_downgrade_survey    plan   469 22688 0.02067172 0.2231436
## 6 awesome_downgrade_survey   media   358 22688 0.01577927 0.2231436
## # ... with 1 more variables: tf_idf &lt;dbl&gt;</code></pre>
<p>The <code>idf</code> and <code>tf_idf</code> will be 0 for common words like “the” and “a”. We’ve already removed these stop words from our dataset.</p>
<p>Let’s visualize these high <code>tf_idf</code> words for each type of churn survey.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The words that appear in these graphs appear more frequently in the specific survey type than they do in the other surveys. I think we can ignore “buffer”. The word “plan” seems to appear often in each survey <em>except</em> the exit survey as well. The term “expensive” appears more often in the <em>business churn survey</em> and the <em>business downgrade awesome survey</em>. It might be worth noting that “media” and “anymore” appear more frequently in the <em>business churn survey</em> than the other surveys.</p>
<p>We don’t have much context and are required to speculate on what the meaning and emotion behind the words might be. It may be beneficial to look at groups of words to help us gather more information. :)</p>
</div>
</div>
<div id="n-grams" class="section level2">
<h2>N-grams</h2>
<p>What if we looked at groups of words instead of just single words? We can check which words tend to appear immediately after another, and which words tend to appear together in the same document.</p>
<p>We’ve been using the <code>unnest_tokens</code> function to tokenize by word, but we can also use the function to tokenize into consecutive sequences of words, called n-grams. By seeing how often word X is followed by word Y, we can then build a model of the relationships between them.</p>
<p>We do this by adding the <code>token = &quot;ngrams&quot;</code> option to <code>unnest_tokens()</code>, and setting <code>n</code> to the number of words we wish to capture in each n-gram. When we set <code>n</code> to 2, we are examining groups of 2 consecutive words, often called “bigrams”:</p>
<pre class="r"><code># Unnest bigrams from responses
bigrams &lt;- responses %&gt;%
  unnest_tokens(bigram, details, token = &quot;ngrams&quot;, n = 2)

# View the bigrams
head(bigrams$bigram)</code></pre>
<pre><code>## [1] &quot;mainly 2&quot;   &quot;2 reasons&quot;  &quot;reasons n1&quot; &quot;n1 the&quot;     &quot;the mobile&quot;
## [6] &quot;mobile app&quot;</code></pre>
<p>Great! Each token now is represented by a bigram. Let’s take a quick look at the most common bigrams</p>
<pre class="r"><code># Count the most common bigrams
bigrams %&gt;%
  count(bigram, sort = TRUE)</code></pre>
<pre><code>## # A tibble: 42,210 x 2
##          bigram     n
##           &lt;chr&gt; &lt;int&gt;
##  1 social media   524
##  2         i am   421
##  3       i have   418
##  4      i don&#39;t   410
##  5      need to   373
##  6    not using   373
##  7     using it   356
##  8        i was   327
##  9       to use   309
## 10       i need   303
## # ... with 42,200 more rows</code></pre>
<p>As we might expect, a lot of the most common bigrams are groups of common words. This is a useful time to use tidyr’s <code>separate()</code>, which splits a column into multiple based on a delimiter. This lets us separate it into two columns, “word1” and “word2”, at which point we can remove cases where either is a stop-word.</p>
<pre class="r"><code># Separate words in bigrams
separated &lt;- bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

# Define our own stop words
word &lt;- c(&quot;i&quot;, &quot;i&#39;m&quot;, &quot;it&quot;, &quot;the&quot;, &quot;at&quot;, &quot;to&quot;, &quot;right&quot;, &quot;just&quot;, &quot;to&quot;, &quot;a&quot;, &quot;an&quot;,
          &quot;that&quot;, &quot;but&quot;, &quot;as&quot;, &quot;so&quot;, &quot;will&quot;, &quot;for&quot;, &quot;longer&quot;, &quot;i&#39;ll&quot;, &quot;of&quot;, &quot;my&quot;,
          &quot;n&quot;, &quot;do&quot;, &quot;did&quot;, &quot;am&quot;, &quot;with&quot;, &quot;been&quot;, &quot;and&quot;, &quot;we&quot;)

# Create tibble of stop words
stopwords &lt;- tibble(word)

# Filter out stop-words
filtered &lt;- separated %&gt;%
  filter(!word1 %in% stopwords$word) %&gt;%
  filter(!word2 %in% stopwords$word)

# Calculate new bigram counts
bigram_counts &lt;- filtered %&gt;% 
  count(word1, word2, sort = TRUE)

head(bigram_counts)</code></pre>
<pre><code>## # A tibble: 6 x 3
##     word1   word2     n
##     &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;
## 1  social   media   524
## 2     not   using   373
## 3   don&#39;t    need   300
## 4      be    back   299
## 5 awesome    plan   207
## 6 another account   171</code></pre>
<p>We’ll use tidyr’s <code>unite()</code> function to recombine the columns into one.</p>
<pre class="r"><code># Reunite the words
bigrams_united &lt;- filtered %&gt;%
  unite(bigram, word1, word2, sep = &quot; &quot;)

head(bigrams_united$bigram)</code></pre>
<pre><code>## [1] &quot;mainly 2&quot;   &quot;2 reasons&quot;  &quot;reasons n1&quot; &quot;mobile app&quot; &quot;app is&quot;    
## [6] &quot;is buggy&quot;</code></pre>
<p>Nice! Let’s look at the most common bigrams.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The most common bigram is “social media”, which makes sense. It’s more interesting to see that the next two most common bigrams are “not using” and “don’t need”. This seems like a clear signal that Buffer wasn’t filling these users’ needs in one way or another, which led them to leaving the product.</p>
<p>Bigrams like “be back”, “another account”, and “have another” indicate that these users either have another Buffer account, or need to stop using it only temporarily.</p>
<p>A bigram can also be treated as a term in a document in the same way that we treated individual words. For example, we can look at the tf-idf of these trigrams across the surveys. These tf-idf values can be visualized within each segment, just as we did for words earlier.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>These bigrams give us slightly more context about what users are thinking in each survey. Responses from the <em>business churn survey</em> and <em>business downgrade survey</em> emphasize how expensive the plan is. They also mention a competitor, Sprout. Responses from the <code>exit survey</code> indicate account confusion.</p>
<p>We may want to visualize the relationship between these bigrams, instead of just listing the most common ones.</p>
</div>
<div id="visualizing-a-network-of-bigrams-with-ggraph" class="section level2">
<h2>Visualizing a network of bigrams with ggraph</h2>
<p>As one common visualization, we can arrange the words into a network, or “graph.” Here we’ll be referring to a “graph” not in the sense of a visualization, but as a combination of connected nodes. A graph can be constructed from a tidy object since it has three variables:</p>
<ul>
<li>from: the node an edge is coming from</li>
<li>to: the node an edge is going towards</li>
<li>weight: A numeric value associated with each edge</li>
</ul>
<p>The <code>igraph</code> package has many powerful functions for manipulating and analyzing networks. One way to create an igraph object from tidy data is the <code>graph_from_data_frame()</code> function, which takes a data frame of edges with columns for “from”, “to”, and edge attributes (in this case n):</p>
<pre class="r"><code>library(igraph)

# Original counts
head(bigram_counts)</code></pre>
<pre><code>## # A tibble: 6 x 3
##     word1   word2     n
##     &lt;chr&gt;   &lt;chr&gt; &lt;int&gt;
## 1  social   media   524
## 2     not   using   373
## 3   don&#39;t    need   300
## 4      be    back   299
## 5 awesome    plan   207
## 6 another account   171</code></pre>
<p>Let’s create a bigram graph object.</p>
<pre class="r"><code># filter for only relatively common combinations
bigram_graph &lt;- bigram_counts %&gt;%
  filter(n &gt; 40) %&gt;%
  graph_from_data_frame()

bigram_graph</code></pre>
<pre><code>## IGRAPH DN-- 73 75 -- 
## + attr: name (v/c), n (e/n)
## + edges (vertex names):
##  [1] social  -&gt;media     not     -&gt;using     don&#39;t   -&gt;need     
##  [4] be      -&gt;back      awesome -&gt;plan      another -&gt;account  
##  [7] thank   -&gt;you       don&#39;t   -&gt;use       using   -&gt;buffer   
## [10] love    -&gt;buffer    buffer  -&gt;account   have    -&gt;another  
## [13] don&#39;t   -&gt;have      use     -&gt;buffer    too     -&gt;expensive
## [16] buffer  -&gt;is        be      -&gt;able      signed  -&gt;up       
## [19] more    -&gt;than      come    -&gt;back      this    -&gt;account  
## [22] is      -&gt;not       this    -&gt;is        you     -&gt;guys     
## + ... omitted several edges</code></pre>
<p>We can convert an igraph object into a ggraph with the ggraph function, after which we add layers to it, much like layers are added in ggplot2. For example, for a basic graph we need to add three layers: nodes, edges, and text.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>This is a visualization of a <strong>Markov chain</strong>, a model in text processing. In a Markov chain, each choice of word depends only on the previous word. In this case, a random generator following this model might spit out “buffer”, then “is”, then “great”, by following each word to the most common words that follow it. To make the visualization interpretable, I chose to show only the most common word to word connections. What can we learn from this graph?</p>
<p>We can use this graph to visualize some details about the text structure. For example, we can see that “buffer” and “account” form the centers of groups of nodes.</p>
<p>We also see pairs or triplets along the outside that form common short phrases (“can’t afford”, “too expensive”, or “don’t need”).</p>
<p>I see that “don’t” is at the center of a cluster of nodes. These include the phrases “don’t need”, “don’t want”, and “don’t have”.</p>
<p>What would this graph look like if we only looked at the responses of the <em>business churn survey</em>?</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>There are similar themes in this graph. Users simply aren’t using, or don’t need, Buffer. The term “not” is the center of a cluster of nodes including “using”, “used”, “enough”, and “needed”.</p>
<p>Cost is mentioned, as well as refunds. Sprout social is mentioned, as well as set up. Interestingly “come back” and “thank you” are also present.</p>
</div>
<div id="change-over-time" class="section level2">
<h2>Change over time</h2>
<p>What words and topics have become more frequent, or less frequent, over time? These could give us a sense of what customers think about Buffer, and how that has changed.</p>
<p>We can first count the number of times each word is used each month, and then use the <code>broom</code> package to fit a logistic regression model to examine whether the frequency of each word increases or decreases over time. Every term will then have a growth rate (as an exponential term) associated with it.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>We can see the term “clients” has become much more frequent over time, especially since the beginning of 2017. This makes sense, as we have shift focus to be more business-focused. Terms about the cost of Buffer have also increased in frequency over time. The terms “cost”, “costs”, and “afford” have become much more frequent as time has gone on.</p>
</div>
<div id="negative-words" class="section level2">
<h2>Negative words</h2>
<p>Words like “not”, “don’t”, “can’t”, and “never” can be considered negative words. One thing we can do is look at the words that occur most frequently <em>after</em> these negative words.</p>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>These plots can be informative for us. The most common terms after the word “not” are “using”, “enough”, “needed”, and “need”.</p>
<p>After the term “no”, “longer” is the most common, suggesting the terms “no longer need” or “no longer using”. After the term “can’t”, “afford” is the most common, followed by “post”, “use”, “find”, and “connect”.</p>
<p>After the word “won’t”, “let”, “use”, “allow”, “word”, “upload”, “start”, “remove”, and “need” appear, which indicates some problems with the product. After the word “doesn’t”, “work” is the most common, followed by “allow”, “do”, and “suppord”. The terms “used”, “signed”, “work” and “worked” appear most frequently after the term “never”.</p>
</div>
<div id="topic-modeling" class="section level2">
<h2>Topic modeling</h2>
<p>In analyzing these survey responses, we feel the urge to group words and responses into natural groups so that we can understand them separately. Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.</p>
<p>Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.</p>
<p>LDA is a mathematical method for finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.</p>
<p>We can use the <code>LDA()</code> function from the <code>topicmodels</code> package, setting k = 2, to create a two-topic LDA model.</p>
<pre class="r"><code>library(topicmodels); library(tm)</code></pre>
<pre><code>## Loading required package: NLP</code></pre>
<pre><code>## 
## Attaching package: &#39;NLP&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     annotate</code></pre>
<pre class="r"><code># Create a text corpus
corpus &lt;- Corpus(VectorSource(text_df$word))

# Created a document term matrix
dtm &lt;- DocumentTermMatrix(corpus)

# Find the sum of words in each Document
rowTotals &lt;- apply(dtm , 1, sum) 

# Remove all docs without words
dtm_new   &lt;- dtm[rowTotals &gt; 0, ]        

inspect(dtm_new)</code></pre>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 41348, terms: 5271)&gt;&gt;
## Non-/sparse entries: 41426/217903882
## Sparsity           : 100%
## Maximal term length: 22
## Weighting          : term frequency (tf)
## Sample             :
##        Terms
## Docs    account accounts buffer business media plan post posts social time
##   115         0        0      0        0     0    0    0     0      0    0
##   17357       0        0      0        0     0    0    0     0      0    0
##   17358       0        0      0        0     0    0    0     0      0    0
##   21          0        0      0        0     0    0    0     0      0    0
##   22          0        0      0        0     0    0    0     0      0    0
##   2327        0        0      0        0     0    0    0     0      0    0
##   255         0        0      0        0     0    0    0     0      0    0
##   344         0        0      0        0     0    0    0     0      0    0
##   345         0        0      0        0     0    0    0     0      0    0
##   714         0        0      0        0     0    0    0     0      0    0</code></pre>
<pre class="r"><code># Set a seed so that the output of the model is predictable
lda &lt;- LDA(dtm_new, k = 2, control = list(seed = 1234))
lda</code></pre>
<pre><code>## A LDA_VEM topic model with 2 topics.</code></pre>
<p>Now that we’ve fit the LDA model, the rest of the analysis will involve exploring and interpreting the model using tidying functions from the <code>tidytext</code> package.</p>
<div id="word-topic-probabilities" class="section level3">
<h3>Word-topic probabilities</h3>
<p>From the Tidy Text Mining with R book:</p>
<blockquote>
<p>…the tidy() method, originally from the broom package (Robinson 2017), for tidying model objects. The tidytext package provides this method for extracting the per-topic-per-word probabilities, called ββ (“beta”), from the model.</p>
</blockquote>
<pre class="r"><code>topics &lt;- tidy(lda, matrix = &quot;beta&quot;)
topics</code></pre>
<pre><code>## # A tibble: 10,542 x 3
##    topic         term         beta
##    &lt;int&gt;        &lt;chr&gt;        &lt;dbl&gt;
##  1     1     integral 1.648001e-05
##  2     2     integral 3.178251e-05
##  3     1     allocate 4.672497e-05
##  4     2     allocate 1.602059e-06
##  5     1        willl 3.451380e-05
##  6     2        willl 1.378719e-05
##  7     1 troubleshoot 2.629537e-05
##  8     2 troubleshoot 2.198808e-05
##  9     1    realising 1.435645e-05
## 10     2    realising 3.390154e-05
## # ... with 10,532 more rows</code></pre>
<p>This has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic.</p>
<p>We could use <code>dplyr</code>’s <code>top_n()</code> to find the 10 terms that are most common within each topic.</p>
<pre class="r"><code># Find top terms
lda_top_terms &lt;- topics %&gt;%
  group_by(topic) %&gt;%
  top_n(15, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

# Plot the results
lda_top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) +
  coord_flip()</code></pre>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>These graphs show the words that are most common in each topic. Notice that there is a lot of overlap – this is a feature of LDA.</p>
<p>As an alternative, we could consider the terms that had the greatest difference in β between topic 1 and topic 2. This can be estimated based on the log ratio of the two: log2(β2/β1) (a log ratio is useful because it makes the difference symmetrical: β2 being twice as large leads to a log ratio of 1, while β1 being twice as large results in -1).</p>
<p>To constrain it to a set of especially relevant words, we can filter for relatively common words, such as those that have a ββ greater than 1/1000 in at least one topic.</p>
<pre class="r"><code># Find the differences in betas
beta_spread &lt;- topics %&gt;%
  mutate(topic = paste0(&quot;topic&quot;, topic)) %&gt;%
  spread(topic, beta) %&gt;%
  filter(topic1 &gt; .001 | topic2 &gt; .001) %&gt;%
  mutate(log_ratio = log2(topic2 / topic1))

# Plot it out
beta_spread %&gt;%
  group_by(log_ratio &lt; 0) %&gt;%
  top_n(15, abs(log_ratio)) %&gt;%
  ungroup() %&gt;%
  mutate(term = reorder(term, log_ratio)) %&gt;%
  ggplot(aes(term, log_ratio, fill = log_ratio &lt; 0)) +
  geom_col() +
  coord_flip() +
  ylab(&quot;Log Odds Ratio&quot;) +
  xlab(&quot;&quot;) +
  scale_fill_discrete(name = &quot;&quot;, labels = c(&quot;Topic 1&quot;, &quot;Topic 2&quot;))</code></pre>
<p><img src="/blog/churn-survey-text-analysis_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>To be honest, I can’t quite figure out how these words relate to eachother and to a single topic. This method might not be the most useful to us in this particular case.</p>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>Based on the results of our anlaysis, it feels important to figure out why users stop using and needing Buffer. In many cases it could be due to external factors like business needs, market forces, layoffs, but in other cases it could be due to Buffer itself. Perhaps Buffer could have a better engagement loop. Or perhaps Buffer could help users that become inactive by suggesting content to share.</p>
<p>Another theme that appears repeatedly is cost. We know that the current pricing structure isn’t completely ideal, so it feels good to be working towards a more individualized structure over the next few product cycles.</p>
<p>There is a general theme of gratitude in these responses - “i love buffer” was a common phrase that appeared often in each survey. It’s comforting to know that people like the product and team – I hope that we’ll be able to use some of these learnings to give them a better experience. :)</p>
<pre class="r"><code># Unload lubridate package
detach(&quot;package:lubridate&quot;, unload=TRUE)</code></pre>
</div>

      </article>

      <ul class="pager blog-pager">
        
          <li class="previous">
            <a href="/blog/content-recommendations/" data-toggle="tooltip" data-placement="top" title="Simple Content Recommendations in R">&larr; Previous Post</a>
          </li>
        
        
          <li class="next">
            <a href="/blog/got-screentimes/" data-toggle="tooltip" data-placement="top" title="Game of Thrones: Who Steals the Show?">Next Post &rarr;</a>
          </li>
        
      </ul>

      
        
      

    </div>
  </div>
</div>

    <footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="mailto:julianwinternheimer@gmail.com" title="Email me">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://www.facebook.com/julian.winternheimer" title="Facebook">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://github.com/jwinternheimer" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/julheimer" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://linkedin.com/in/jwinternheimer" title="LinkedIn">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            <a href="/index.xml" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          Julian Winternheimer
          &nbsp;&bull;&nbsp;
          2017

          
            &nbsp;&bull;&nbsp;
            <a href="/">Data Analysis?</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="http://gohugo.io">Hugo v0.23</a> powered &nbsp;&bull;&nbsp; Theme by <a href="http://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a> adapted to <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a>
          
        </p>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha384-dq1/gEHSxPZQ7DdrM82ID4YVol9BYyU7GbWlIwnwyPzotpoc57wDw/guX8EaYGPx" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="/js/main.js"></script>
<script src="/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> renderMathInElement(document.body); </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe.min.js" integrity="sha256-UplRCs9v4KXVJvVY+p+RSo5Q4ilAUXh7kpjyIP5odyc=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.1/photoswipe-ui-default.min.js" integrity="sha256-PWHOlUzc96pMc8ThwRIXPn8yH4NOLu42RQ0b9SpnpFk=" crossorigin="anonymous"></script>
<script src="/js/load-photoswipe.js"></script>




  </body>
</html>

